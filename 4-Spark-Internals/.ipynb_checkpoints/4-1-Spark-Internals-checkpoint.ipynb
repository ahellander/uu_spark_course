{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Spark\n",
    "\n",
    "This part is for the interested student, looking at the importance of tuning Spark for best performance. By design, much of the performance tuning can be done from the application level, e.g. in making the right choices for partitioning and methods used (avoiding shuffling as much as possible). Understanding the RDD DAG â€¦\n",
    "\n",
    "More and more of the tunining is done 'under the hood' in Spark, e.g. in Python, stored objects will always be serialized with the Pickle library, so it does not matter whether you choose a serialized level (not the case for Java and Scala on Spark). Spark also automatically persists some intermediate data in shuffle operations (e.g. reduceByKey), even without users calling persist. This is done to avoid recomputing the entire input if a node fails during the shuffle. We still recommend users call persist on the resulting RDD if they plan to reuse it.\n",
    "\n",
    "There are more about tuning Spark, with a good starting point here: https://spark.apache.org/docs/latest/tuning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initiate Spark Context - ONLY first time for each notebook. If you get problems with below, see [Help](/notebooks/spark_course/1-Course-Information-and-Links/If-you-get-problems-initiating-spark-context.ipynb)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(appName=\"search\", master=os.environ['MASTER'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data and cache it (we don't want to measure HDFS performance this time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = sc.textFile(\"/uuData/names\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so many names: 427500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Naive way, using groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10979700089\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "sc.textFile(\"/uuData/names\").map(lambda name : (name[0], name)).groupByKey().mapValues(lambda name: len(set(name))).collect()\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we already had the names cached (look above '.cache()', so let's see what difference that does the time for counting names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.73540711403\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn.map(lambda name : (name[0], name)).groupByKey().mapValues(lambda name: len(set(name))).collect()\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems that for this size of data it is something else that takes more time. Let's break it up in steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000255823135376\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn.map(lambda name : (name[0], name))\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0273609161377\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn.map(lambda name : (name[0], name)).groupByKey()\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030730009079\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn.map(lambda name : (name[0], name)).groupByKey().mapValues(lambda name: len(set(name)))\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.60329914093\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn.map(lambda name : (name[0], name)).groupByKey().mapValues(lambda name: len(set(name))).collect()\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the collect step is the major part in above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) (as before) avoid groupByKey, and 2) use partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.67960309982\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "sc.textFile(\"/uuData/names\").distinct(numPartitions = 1).map(lambda name :  (name[0], 1)).reduceByKey(lambda a, b: a + b).collect()\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the number of partitions to 2 (more doesn't help on this small dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.62720608711\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "sc.textFile(\"/uuData/names\").distinct(numPartitions = 2).map(lambda name :  (name[0], 1)).reduceByKey(lambda a, b: a + b).collect()\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we already had the names cached (look above '.cache()', so let's see what difference that does the time for counting names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.40223312378\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn.distinct(numPartitions = 1).map(lambda name :  (name[0], 1)).reduceByKey(lambda a, b: a + b).collect()\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with partition 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.44529819489\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn.distinct(numPartitions = 2).map(lambda name :  (name[0], 1)).reduceByKey(lambda a, b: a + b).collect()\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From ? to ? seconds, from using no caching, groupByKey and no partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (if there's time for it)\n",
    "So in above case the importance of tuning gave us x(?) speedup. Create a larger data set, and try above again (try different partitions etc). See next notebook."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Material based on AMPCamp and Databricks training material provided online under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
