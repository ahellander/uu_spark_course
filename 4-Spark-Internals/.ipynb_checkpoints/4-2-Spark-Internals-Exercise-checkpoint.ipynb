{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitions matter!\n",
    "\n",
    "This part is for the interested student, looking at the importance of tuning Spark for best performance. By design, much of the performance tuning can be done from the application level, e.g. in making the right choices for partitioning and methods used (avoiding shuffling as much as possible). Understanding the RDD DAG â€¦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initiate Spark Context - ONLY first time for each notebook. If you get problems with below, see [Help](/notebooks/spark_course/1-Course-Information-and-Links/If-you-get-problems-initiating-spark-context.ipynb)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(appName=\"search\", master=os.environ['MASTER'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (if there's time for it)\n",
    "So in above case the importance of tuning gave us x(?) speedup. Create a larger data set, and try above again (try different partitions etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a larger dataset out of the current one, using e.g. below, then put it into HDFS and read it into Spark (note: you can use something else than '100'):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data and cache it (we don't want to measure HDFS performance this time)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd /home/ubuntu/notebooks/spark_course/data\n",
    "for i in {1..10}; do cat names >> namesMedium; done\n",
    "for i in {1..100}; do cat names >> namesLarge; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "namesMedium is about 29M if you use '1..10' in above. \n",
    "namesLarge is about 290M if you use '1..100' in above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then put it into HDFS."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hdfs dfs -put namesMedium /uuData/\n",
    "hdfs dfs -put namesLarge /uuData/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that it's there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 items\n",
      "-rw-r--r--   1 ubuntu supergroup        534 2015-04-17 08:23 /uuData/README.md\n",
      "-rw-r--r--   1 ubuntu supergroup     174449 2015-04-17 08:16 /uuData/access_log\n",
      "-rw-r--r--   1 ubuntu supergroup      14989 2015-04-17 08:16 /uuData/error_log\n",
      "-rw-r--r--   1 ubuntu supergroup     197105 2015-04-17 08:16 /uuData/lr_data.txt\n",
      "drwxr-xr-x   - ubuntu supergroup          0 2015-04-17 08:16 /uuData/movies\n",
      "-rw-r--r--   1 ubuntu supergroup    3004200 2015-04-17 08:16 /uuData/names\n",
      "-rw-r--r--   1 ubuntu supergroup  303424200 2015-04-17 14:13 /uuData/namesLarge\n",
      "-rw-r--r--   1 ubuntu supergroup   30042000 2015-04-17 14:26 /uuData/namesMedium\n",
      "drwxr-xr-x   - ubuntu supergroup          0 2015-04-17 08:16 /uuData/pagecounts\n",
      "-rw-r--r--   1 ubuntu supergroup         73 2015-04-17 08:16 /uuData/people.json\n",
      "-rw-r--r--   1 ubuntu supergroup         32 2015-04-17 08:16 /uuData/people.txt\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /uuData/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with namesMedium. Try namesLarge or other when/if you get time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = sc.textFile(\"/uuData/namesMedium\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More names now: 4275000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4275000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Naive way, using groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.03503704071\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "sc.textFile(\"/uuData/names\").map(lambda name : (name[0], name)).groupByKey().mapValues(lambda name: len(set(name))).collect()\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we already had the names cached (look above '.cache()', so let's see what difference that does the time for counting names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.2072148323\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn.map(lambda name : (name[0], name)).groupByKey().mapValues(lambda name: len(set(name))).collect()\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems that for this size of data it is something else that takes more time. Let's break it up in steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000245094299316\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn.map(lambda name : (name[0], name))\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0188281536102\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn.map(lambda name : (name[0], name)).groupByKey()\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0321490764618\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn.map(lambda name : (name[0], name)).groupByKey().mapValues(lambda name: len(set(name)))\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.9009840488\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn.map(lambda name : (name[0], name)).groupByKey().mapValues(lambda name: len(set(name))).collect()\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) (as before) avoid groupByKey, and 2) use partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.51583003998\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "sc.textFile(\"/uuData/names\").distinct(numPartitions = 1).map(lambda name :  (name[0], 1)).reduceByKey(lambda a, b: a + b).collect()\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the number of partitions to 2 (more doesn't help on this small dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.50060105324\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "sc.textFile(\"/uuData/names\").distinct(numPartitions = 2).map(lambda name :  (name[0], 1)).reduceByKey(lambda a, b: a + b).collect()\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we already had the names cached (look above '.cache()', so let's see what difference that does the time for counting names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1247398853\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn.distinct(numPartitions = 1).map(lambda name :  (name[0], 1)).reduceByKey(lambda a, b: a + b).collect()\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now with partition 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2350699902\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn.distinct(numPartitions = 8).map(lambda name :  (name[0], 1)).reduceByKey(lambda a, b: a + b).collect()\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From ? to ? seconds, from using no caching, groupByKey and no partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any new tuning effects?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Material based on AMPCamp and Databricks training material provided online under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
