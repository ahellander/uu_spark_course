{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCount, now with more data\n",
    "\n",
    "In this section we'll use a sligthly larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initiate Spark Context - ONLY first time for each notebook. If you get problems with below, see [Help](/notebooks/spark_course/1-Course-Information-and-Links/If-you-get-problems-initiating-spark-context.ipynb)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(appName=\"search\", master=os.environ['MASTER'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what data we have to play with in HDFS (note: the '%%sh' gives you the shell environment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxr-xr-x   - ubuntu supergroup          0 2015-04-18 06:11 /user\n",
      "drwxr-xr-x   - ubuntu supergroup          0 2015-04-18 12:22 /uuData\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\n",
      "-rw-r--r--   1 ubuntu supergroup        534 2015-04-18 12:22 /uuData/README.md\n",
      "-rw-r--r--   1 ubuntu supergroup     174449 2015-04-18 06:26 /uuData/access_log\n",
      "-rw-r--r--   1 ubuntu supergroup      14989 2015-04-18 06:26 /uuData/error_log\n",
      "-rw-r--r--   1 ubuntu supergroup     197105 2015-04-18 06:26 /uuData/lr_data.txt\n",
      "drwxr-xr-x   - ubuntu supergroup          0 2015-04-18 12:14 /uuData/movies\n",
      "-rw-r--r--   1 ubuntu supergroup    3004200 2015-04-18 06:26 /uuData/names\n",
      "drwxr-xr-x   - ubuntu supergroup          0 2015-04-18 06:26 /uuData/pagecounts\n",
      "-rw-r--r--   1 ubuntu supergroup         73 2015-04-18 06:26 /uuData/people.json\n",
      "-rw-r--r--   1 ubuntu supergroup         32 2015-04-18 06:26 /uuData/people.txt\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /uuData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now point read the data into pagecounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFile = \"/uuData/pagecounts\"\n",
    "pagecounts = sc.textFile(dataFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/uuData/pagecounts MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagecounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the data. You can use the take operation of an RDD to get the first K records. Here, K = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'20090505-000000 aa Main_Page 2 9980',\n",
       " u'20090505-000000 ab %D0%90%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%BD%D0%B5%D1%82 1 465',\n",
       " u'20090505-000000 ab %D0%98%D1%85%D0%B0%D0%B4%D0%BE%D1%83_%D0%B0%D0%B4%D0%B0%D2%9F%D1%8C%D0%B0 1 16086',\n",
       " u'20090505-000000 af.b Tuisblad 1 36236',\n",
       " u'20090505-000000 af.d Tuisblad 4 189738',\n",
       " u'20090505-000000 af.q Tuisblad 2 56143',\n",
       " u'20090505-000000 af Afrika 1 46833',\n",
       " u'20090505-000000 af Afrikaans 2 53577',\n",
       " u'20090505-000000 af Australi%C3%AB 1 132432',\n",
       " u'20090505-000000 af Barack_Obama 1 23368']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagecounts.take(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this is not very readable because take() returns an array and Scala simply prints the array with each element separated by a comma. We can make it prettier by traversing the array to print each record on its own line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20090505-000000 aa Main_Page 2 9980\n",
      "20090505-000000 ab %D0%90%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%BD%D0%B5%D1%82 1 465\n",
      "20090505-000000 ab %D0%98%D1%85%D0%B0%D0%B4%D0%BE%D1%83_%D0%B0%D0%B4%D0%B0%D2%9F%D1%8C%D0%B0 1 16086\n",
      "20090505-000000 af.b Tuisblad 1 36236\n",
      "20090505-000000 af.d Tuisblad 4 189738\n",
      "20090505-000000 af.q Tuisblad 2 56143\n",
      "20090505-000000 af Afrika 1 46833\n",
      "20090505-000000 af Afrikaans 2 53577\n",
      "20090505-000000 af Australi%C3%AB 1 132432\n",
      "20090505-000000 af Barack_Obama 1 23368\n"
     ]
    }
   ],
   "source": [
    "for x in pagecounts.take(10):\n",
    "    print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Let's see how many records in total are in this data set (this command will take a while, so read ahead while it is running)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1398882"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagecounts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   This should launch tasks on the your Spark cluster.\n",
    "   If you look closely at the terminal, the console log is pretty chatty and tells you the progress of the tasks.\n",
    "\n",
    "   While it's running, you can open the Spark web console to see the progress.\n",
    "   To do this, open your favorite browser, and type in the following URL.\n",
    "\n",
    "   `http://<the public IP you're using right now>:4040`\n",
    "\n",
    "   Note that this page is only available if you have an active job or Spark shell.  \n",
    "   You should see the Spark application status web interface, similar to the following:\n",
    "\n",
    "   ![Spark Application Status Web UI](images/stages.jpg)\n",
    "\n",
    "   The links in this interface allow you to track the job's progress and\n",
    "   various metrics about its execution, including task durations and cache\n",
    "   statistics.\n",
    "\n",
    "   In addition, the Spark Standalone cluster status web interface displays\n",
    "   information that pertains to the entire Spark cluster.  To view this UI,\n",
    "   browse to\n",
    "\n",
    "   `http://<the public IP you're using right now>:8080`\n",
    "\n",
    "   You should see a page similar to the following:\n",
    "\n",
    "   ![Spark Cluster Status Web UI](images/sparkmaster.jpg)\n",
    "\n",
    "   When your query finishes running, it should return the following count:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1398882"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Recall from above when we described the format of the data set, that the second field is the \"project code\" and contains information about the language of the pages.\n",
    "   For example, the project code \"en\" indicates an English page.\n",
    "   Let's derive an RDD containing only English pages from `pagecounts`.\n",
    "   This can be done by applying a filter function to `pagecounts`.\n",
    "   For each record, we can split it by the field delimiter (i.e. a space) and get the second field-â€“ and then compare it with the string \"en\".\n",
    "\n",
    "   To avoid reading from disks each time we perform any operations on the RDD, we also __cache the RDD into memory__.\n",
    "    This is where Spark really starts to to shine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enPages = pagecounts.filter(lambda x: x.split(\" \")[1] == \"en\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you type this command into the Spark shell, Spark defines the RDD, but because of lazy evaluation, no computation is done yet.\n",
    "   Next time any action is invoked on `enPages`, Spark will cache the data set in memory across the workers in your cluster.\n",
    "\n",
    "5. How many records are there for English pages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "970545"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enPages.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   The first time this command is run, similar to the last count we did, it will take 2 - 3 minutes while Spark scans through the entire data set on disk.\n",
    "   __But since enPages was marked as \"cached\" in the previous step, if you run count on the same RDD again, it should return an order of magnitude faster__.\n",
    "\n",
    "   If you examine the console log closely, you will see lines like this, indicating some data was added to the cache:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13/02/05 20:29:01 INFO storage.BlockManagerMasterActor$BlockManagerInfo: Added rdd_2_172 in memory on ip-10-188-18-127.ec2.internal:42068 (size: 271.8 MB, free: 5.5 GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Let's try something fancier.\n",
    "   Generate a histogram of total page views on Wikipedia English pages for the date range represented in our dataset (May 5 to May 7, 2009).\n",
    "   The high level idea of what we'll be doing is as follows.\n",
    "   First, we generate a key value pair for each line; the key is the date (the first eight characters of the first field), and the value is the number of pageviews for that date (the fourth field)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enTuples = enPages.map(lambda x: x.split(\" \"))\n",
    "enKeyValuePairs = enTuples.map(lambda x: (x[0][:8], int(x[3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shuffle the data and group all values of the same key together.\n",
    "   Finally we sum up the values for each key.\n",
    "   There is a convenient method called `reduceByKey` in Spark for exactly this pattern.\n",
    "   Note that the second argument to `reduceByKey` determines the number of reducers to use.\n",
    "   By default, Spark assumes that the reduce function is commutative and associative and applies combiners on the mapper side.\n",
    "   Since we know there is a very limited number of keys in this case (because there are only 3 unique dates in our data set), let's use only one reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'20090507', 6175726), (u'20090505', 7076855)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enKeyValuePairs.reduceByKey(lambda x, y: x + y, 1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `collect` method at the end converts the result from an RDD to an array.\n",
    "We can combine the previous three commands into one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'20090507', 6175726), (u'20090505', 7076855)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enPages.map(lambda x: x.split(\" \")).map(lambda x: (x[0][:8], int(x[3]))).reduceByKey(lambda x, y: x + y, 1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Suppose we want to find pages that were viewed more than 200,000 times during the three days covered by our dataset.\n",
    "   Conceptually, this task is similar to the previous query.\n",
    "   But, given the large number of pages (23 million distinct page names), the new task is very expensive.\n",
    "   We are doing an expensive group-by with a lot of network shuffling of data.\n",
    "\n",
    "   To recap, first we split each line of data into its respective fields.\n",
    "   Next, we extract the fields for page name and number of page views.\n",
    "   We reduce by key again, this time with 40 reducers.\n",
    "   Then we filter out pages with less than 200,000 total views over our time window represented by our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(451126, u'Main_Page'), (1066734, u'404_error/'), (468159, u'Special:Search')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enPages.map(lambda x: x.split(\" \")).map(lambda x: (x[2], int(x[3]))).reduceByKey(lambda x, y: x + y, 40).filter(lambda x: x[1] > 200000).map(lambda x: (x[1], x[0])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   There is no hard and fast way to calculate the optimal number of reducers for a given problem; you will build up intuition over time by experimenting with different values."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Material based on AMPCamp and Databricks training material provided online under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
