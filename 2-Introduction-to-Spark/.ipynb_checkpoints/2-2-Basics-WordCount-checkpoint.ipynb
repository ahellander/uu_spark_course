{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic WordCount\n",
    "\n",
    "In this section we will learn some of the ways you can explore text data using Spark. In the next session we'll use a larger dataset (still not very large - to save time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initiate Spark Context - ONLY first time for each notebook. If you get problems with below, see [Help](/notebooks/spark_course/1-Course-Information-and-Links/If-you-get-problems-initiating-spark-context.ipynb)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(appName=\"search\", master=os.environ['MASTER'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Basic WordCount example\n",
    "'Basic' in the sense that we're only playing with a local file. In the next notebook we'll use some more data from HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step0: Load some textfile into HDFS\n",
    "Let's find some text file to play with using '%%sh' (which gives you the shell environment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Course-Information-and-Links\n",
      "2-Introduction-to-Spark\n",
      "3-Spark-SQL-and-Dataframes\n",
      "4-Spark-Internals\n",
      "5-Spark-Streaming\n",
      "6-MLlib-Example\n",
      "7-Solutions-to-exercises\n",
      "loadData.sh\n",
      "personalRatings.txt\n",
      "README.md\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "ls ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the README.md into HDFS (and check that it's there):\n",
    "\n",
    "*Note: it might be that README.md is already in HDFS, no problems with that.* \n",
    "\n",
    "*Note: feel free to add some other file. In the next session we'll use a larger file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs dfs -put -f ../README.md /uuData/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\n",
      "-rw-r--r--   1 ubuntu supergroup        534 2015-04-18 12:08 /uuData/README.md\n",
      "-rw-r--r--   1 ubuntu supergroup     174449 2015-04-18 06:26 /uuData/access_log\n",
      "-rw-r--r--   1 ubuntu supergroup      14989 2015-04-18 06:26 /uuData/error_log\n",
      "-rw-r--r--   1 ubuntu supergroup     197105 2015-04-18 06:26 /uuData/lr_data.txt\n",
      "drwxr-xr-x   - ubuntu supergroup          0 2015-04-18 06:26 /uuData/movies\n",
      "-rw-r--r--   1 ubuntu supergroup    3004200 2015-04-18 06:26 /uuData/names\n",
      "drwxr-xr-x   - ubuntu supergroup          0 2015-04-18 06:26 /uuData/pagecounts\n",
      "-rw-r--r--   1 ubuntu supergroup         73 2015-04-18 06:26 /uuData/people.json\n",
      "-rw-r--r--   1 ubuntu supergroup         32 2015-04-18 06:26 /uuData/people.txt\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /uuData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step1: Load the text file\n",
    "\n",
    "Now point read the README file into the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filePath = \"/uuData/README.md\"\n",
    "lines = sc.textFile(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/uuData/README.md MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display first 10 lines in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this is not very readable because take() returns an array and Scala simply prints the array with each element separated by a comma. We can make it prettier by traversing the array to print each record on its own line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# spark_course\n",
      "\n",
      "Spark Course for Uppsala University\n",
      "April 21, 2015\n",
      "Ake Edlund and Izhar ul Hassan\n",
      "\n",
      "Material based on AMPCamp and Databricks training material provided online under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in lines.take(10):\n",
    "    print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 2: Inspect the number of partitions (workers) used to store the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions (workers) storing the dataset = 2\n"
     ]
    }
   ],
   "source": [
    "numPartitions = lines.getNumPartitions() # get the number of partitions\n",
    "print \"Number of partitions (workers) storing the dataset = %d\" % numPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 3: Split each line into a list of words separated by a space from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'#',\n",
       " u'spark_course',\n",
       " u'',\n",
       " u'Spark',\n",
       " u'Course',\n",
       " u'for',\n",
       " u'Uppsala',\n",
       " u'University',\n",
       " u'April',\n",
       " u'21,']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = lines.flatMap(lambda x: x.split(' ')) # split each line into a list of words\n",
    "words.take(10) # display the first 10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 4: Filter the list of words to exclude common stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'#',\n",
       " u'spark_course',\n",
       " u'Spark',\n",
       " u'Course',\n",
       " u'for',\n",
       " u'Uppsala',\n",
       " u'University',\n",
       " u'April',\n",
       " u'21,',\n",
       " u'2015']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords = ['','a','*','and','is','of','the','a'] # define the list of stop words\n",
    "filteredWords = words.filter(lambda x: x.lower() not in stopWords) # filter the words\n",
    "filteredWords.take(10) # display the first 10 filtered words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 5: Cache the filtered dataset in memory to speed up future actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:42"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filteredWords.cache() # cache filtered dataset into memory across the cluster worker nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 6: Transform filtered words into list of (word,1) tuples for WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'#', 1),\n",
       " (u'spark_course', 1),\n",
       " (u'Spark', 1),\n",
       " (u'Course', 1),\n",
       " (u'for', 1),\n",
       " (u'Uppsala', 1),\n",
       " (u'University', 1),\n",
       " (u'April', 1),\n",
       " (u'21,', 1),\n",
       " (u'2015', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1Tuples = filteredWords.map(lambda x: (x, 1)) # map the words into (word,1) tuples\n",
    "word1Tuples.take(10) # display the (word,1) tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 7: Aggregate the (word,1) tuples into (word,count) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'From', 1),\n",
       " (u'scratch,', 1),\n",
       " (u'21,', 1),\n",
       " (u'Attribution-NonCommercial-NoDerivatives', 1),\n",
       " (u'#', 1),\n",
       " (u'based', 1),\n",
       " (u'for', 1),\n",
       " (u'AMPCamp', 1),\n",
       " (u'-', 1),\n",
       " (u'dfs', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountTuples = word1Tuples.reduceByKey(lambda x, y: x + y) # aggregate counts for each word\n",
    "wordCountTuples.take(10) # display the first 10 (word,count) tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Display the top 10 (word,count) tuples by count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'spark_course', 2)\n",
      "(u'cd', 2)\n",
      "(u'on', 2)\n",
      "(u'time', 2)\n",
      "(u'***', 2)\n",
      "(u'From', 1)\n",
      "(u'scratch,', 1)\n",
      "(u'21,', 1)\n",
      "(u'Attribution-NonCommercial-NoDerivatives', 1)\n",
      "(u'#', 1)\n"
     ]
    }
   ],
   "source": [
    "sortedWordCountTuples = wordCountTuples.top(10,key=lambda (x, y): y) # top 10 (word,count) tuples\n",
    "for tuple in sortedWordCountTuples: # display the top 10 (word,count) tuples by count \n",
    "  print str(tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####See SQL section for more..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Material based on AMPCamp and Databricks training material provided online under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
