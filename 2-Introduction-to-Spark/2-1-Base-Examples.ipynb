{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initiate Spark Context - ONLY first time for each notebook. If you get problems with below, see [Help](/notebooks/spark_course/1-Course-Information-and-Links/If-you-get-problems-initiating-spark-context.ipynb)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(appName=\"search\", master=os.environ['MASTER'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Interactive Python Shell in the Browser**\n",
    "A Python cell allows you to execute arbitrary Python commands just like in any Python shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of 1 and 1 is 2\n"
     ]
    }
   ],
   "source": [
    "print \"The sum of 1 and 1 is %s\" % (1+1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Notie: you can at any time click in the cell and make your own changes. Add cells when needed, e.g. when you want to try out something new, or breaking up some code into many steps. IPyNB is a de facto IDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Python libraries can be imported:\n",
    "For other libraries that are not available by default, you can upload other libraries to the Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "m = re.search('(?<=abc)def', 'abcdef')\n",
    "m.group(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: This means that you at any time can import what you need from python numerical libraries, matplotlib etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Spark Examples\n",
    "These examples give a quick overview of the Spark API (\"translated\" to Interactive Python Notebook from the Spark web site (http://spark.apache.org/examples.html)). Spark is built on the concept of distributed datasets, which contain arbitrary Java or Python objects. You create a dataset from external data, then apply parallel operations to it. There are two types of operations: transformations, which define a new dataset based on previous ones, and actions, which kick off a job to execute on a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('world', 1), ('again', 1), ('hello', 2), ('goodbye', 2)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sc.parallelize([\"hello\", \"world\", \"goodbye\",\"goodbye\", \"hello\", \"again\"])\n",
    "wordcounts = words.map(lambda s: (s, 1)).reduceByKey(lambda a, b : a + b).collect()\n",
    "wordcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Exercise 1: Calculate the number of unique words in the \"words\" rdd here.\n",
    "(Hint: The answer should be 4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write your code here\n",
    "wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "Link to [Solution pages](/notebooks/spark_course/7-Solutions-to-exercises/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Exercise 2: Create an rdd of numbers, and use Spark to find the mean.\n",
    "(Hint: Use reduce to sum all the numbers and divide by the count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50006752868025783"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write your code here\n",
    "import numpy.random\n",
    "data = numpy.random.rand(1,10000000).flatten()\n",
    "numpy.mean(data)\n",
    "numbers = sc.parallelize(data)\n",
    "ct = numbers.count()\n",
    "numbers.reduce(lambda a,b: a+b)/ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "Link to [Solution pages](/notebooks/spark_course/7-Solutions-to-exercises/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Text Search (update)\n",
    "In this example, we search through the error messages in a log file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "file = sc.textFile(\"/uuData/error_log\")\n",
    "errors = file.filter(lambda line: \"error\" in line)\n",
    "# Count all the errors\n",
    "print errors.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Count errors mentioning MySQL\n",
    "print errors.filter(lambda line: \"File does not exist\" in line).count()\n",
    "# Fetch the MySQL errors as an array of strings\n",
    "print errors.filter(lambda line: \"MySQL\" in line).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##In-Memory Text Search\n",
    "Spark can cache datasets in memory to speed up reuse. In the example above, we can load just the error messages in RAM using:\n",
    "\n",
    "errors.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the first action that uses errors, later ones will be much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Word Count\n",
    "In this example, we use a few more transformations to build a dataset of (String, Int) pairs called counts and then save it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = sc.textFile(\"/uuData/access_log\")\n",
    "counts = file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Estimating Pi\n",
    "Spark can also be used for compute-intensive tasks. This code estimates π by \"throwing darts\" at a circle. We pick random points in the unit square ((0, 0) to (1,1)) and see how many fall in the unit circle. The fraction should be π / 4, so we use this to get our estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partitions = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.138500\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "n = 100000 * partitions\n",
    "\n",
    "def f(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "\n",
    "count = sc.parallelize(xrange(1, n + 1), partitions).map(f).reduce(add)\n",
    "print \"Pi is roughly %f\" % (4.0 * count / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Logistic Regression\n",
    "This is an iterative machine learning algorithm that seeks to find the best hyperplane that separates two sets of points in a multi-dimensional feature space. It can be used to classify messages into spam vs non-spam, for example. Because the algorithm applies the same MapReduce operation repeatedly to the same dataset, it benefits greatly from caching the input in RAM across iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterations = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A logistic regression implementation that uses NumPy (http://www.numpy.org)\n",
    "to act on batches of input data using efficient matrix operations.\n",
    "In practice, one may prefer to use the LogisticRegression algorithm in\n",
    "MLlib, as shown in examples/src/main/python/mllib/logistic_regression.py.\n",
    "\"\"\"\n",
    "\n",
    "from collections import namedtuple\n",
    "from math import exp\n",
    "from os.path import realpath\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "D = 10  # Number of dimensions\n",
    "\n",
    "\n",
    "# Read a batch of points from the input file into a NumPy matrix object. We operate on batches to\n",
    "# make further computations faster.\n",
    "# The data file contains lines of the form <label> <x1> <x2> ... <xD>. We load each block of these\n",
    "# into a NumPy array of size numLines * (D + 1) and pull out column 0 vs the others in gradient().\n",
    "def readPointBatch(iterator):\n",
    "    strs = list(iterator)\n",
    "    matrix = np.zeros((len(strs), D + 1))\n",
    "    for i in xrange(len(strs)):\n",
    "        matrix[i] = np.fromstring(strs[i].replace(',', ' '), dtype=np.float32, sep=' ')\n",
    "    return [matrix]\n",
    "\n",
    "\n",
    "points = sc.textFile(\"/uuData/lr_data.txt\").mapPartitions(readPointBatch).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial w: [-0.12481223 -0.99055981 -0.77579258  0.33652082  0.22640931  0.03879368\n",
      "  0.90980746  0.92434431 -0.85749732  0.39375674]\n",
      "On iteration 1\n",
      "On iteration 2\n",
      "On iteration 3\n",
      "On iteration 4\n",
      "On iteration 5\n",
      "On iteration 6\n",
      "On iteration 7\n",
      "On iteration 8\n",
      "On iteration 9\n",
      "On iteration 10\n",
      "On iteration 11\n",
      "On iteration 12\n",
      "On iteration 13\n",
      "On iteration 14\n",
      "On iteration 15\n",
      "On iteration 16\n",
      "On iteration 17\n",
      "On iteration 18\n",
      "On iteration 19\n",
      "On iteration 20\n",
      "Final w: [ 505.71631611  643.38593344  613.49460251  397.34343394  470.52502102\n",
      "  507.92458279  330.25231094  389.52729754  616.64854737  442.3351731 ]\n"
     ]
    }
   ],
   "source": [
    "# Initialize w to a random value\n",
    "w = 2 * np.random.ranf(size=D) - 1\n",
    "print \"Initial w: \" + str(w)\n",
    "\n",
    "# Compute logistic regression gradient for a matrix of data points\n",
    "def gradient(matrix, w):\n",
    "    Y = matrix[:, 0]    # point labels (first column of input file)\n",
    "    X = matrix[:, 1:]   # point coordinates\n",
    "    # For each point (x, y), compute gradient function, then sum these up\n",
    "    return ((1.0 / (1.0 + np.exp(-Y * X.dot(w))) - 1.0) * Y * X.T).sum(1)\n",
    "\n",
    "def add(x, y):\n",
    "    x += y\n",
    "    return x\n",
    "\n",
    "for i in range(iterations):\n",
    "    print \"On iteration %i\" % (i + 1)\n",
    "    w -= points.map(lambda m: gradient(m, w)).reduce(add)\n",
    "\n",
    "print \"Final w: \" + str(w)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Material based on AMPCamp and Databricks training material provided online under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
